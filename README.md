# SparseFlow

SparseFlow is a plug-and-play **sparse inference acceleration library for Spiking Neural Networks (SNNs)**. It exploits the natural high sparsity of LIF neuron spike outputs â€” skipping all-zero blocks entirely â€” to deliver substantial speedup and energy savings with zero accuracy loss.

## Quick Start

```python
import sparseflow

model = sparseflow.optimize(model)  # That's it. All eligible ops are replaced.
```

## Why SparseFlow?

SNN neurons (LIF, IF, etc.) produce binary spike outputs where **97â€“99% of spatial blocks are all-zero** (verified on ResNet18/34/50/101 with CIFAR-10/100). Yet standard dense operators â€” including cuDNN â€” perform full computation on these zero blocks, wasting compute and energy.

SparseFlow fixes this with a two-stage Triton kernel design:

```
Input spike tensor â”€â”€â–º Stage-1: Prescan â”€â”€â–º Stage-2: Sparse Conv
                        (lightweight scan,    (only non-zero blocks
                         build nz_idx list)    touch the ALU)
```

**Measured sparsity on Spiking-ResNets (Poisson encoding, CIFAR-10, 224Ã—224):**

| Model | Avg Sparsity | Zero-Block Ratio (Block=16) |
|-------|-------------|----------------------------|
| ResNet-18 | 99.06% | 97.90% |
| ResNet-34 | 99.93% | 99.81% |
| ResNet-50 | 99.93% | 99.83% |
| ResNet-101 | 99.97% | 99.93% |

## Architecture

```
sparseflow/
â”œâ”€â”€ __init__.py                  # Top-level API: sparseflow.optimize(model)
â”‚
â”œâ”€â”€ core/                        # Automatic operator replacement framework
â”‚   â”œâ”€â”€ registry.py              #   Spike op registry (LIF, IF, ParametricLIF, ...)
â”‚   â”œâ”€â”€ analyzer.py              #   Network topology analysis: find spike â†’ conv pairs
â”‚   â””â”€â”€ replacer.py              #   Module replacement: swap nn.Conv2d â†’ SparseConv2d
â”‚
â”œâ”€â”€ kernels/                     # Triton GPU kernels (two-stage: prescan + sparse compute)
â”‚   â”œâ”€â”€ conv2d.py                #   3Ã—3 and 1Ã—1 sparse convolution with real weights
â”‚   â”œâ”€â”€ linear.py                #   Sparse fully-connected (TODO)
â”‚   â”œâ”€â”€ depthwise.py             #   Sparse depthwise convolution (TODO)
â”‚   â””â”€â”€ attention.py             #   Sparse multi-head attention (TODO)
â”‚
â”œâ”€â”€ ops/                         # nn.Module wrappers (drop-in replacements for PyTorch ops)
â”‚   â”œâ”€â”€ sparse_conv2d.py         #   SparseConv2d â€” replaces torch.nn.Conv2d
â”‚   â”œâ”€â”€ sparse_linear.py         #   SparseLinear (TODO)
â”‚   â””â”€â”€ sparse_attention.py      #   SparseAttention (TODO)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ block_selector.py        #   Auto block size selection (Hâ‰¥56â†’16, Hâ‰¥14â†’8, Hâ‰¤7â†’skip)
â”‚   â””â”€â”€ profiler.py              #   Hook-based latency / sparsity profiling
â”‚
â””â”€â”€ benchmark/
    â””â”€â”€ test_correctness.py      #   Numerical correctness: sparse vs F.conv2d
```

**Data flow through the stack:**

```
sparseflow.optimize(model)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Registry â”‚ â”€â”€â–º â”‚ Analyzer â”‚ â”€â”€â–º â”‚ Replacer â”‚
â”‚ (which   â”‚     â”‚ (find    â”‚     â”‚ (swap    â”‚
â”‚  ops are â”‚     â”‚  spike â†’ â”‚     â”‚  Conv2d  â”‚
â”‚  spikes) â”‚     â”‚  conv)   â”‚     â”‚  in-placeâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                                 SparseConv2d (ops/)
                                       â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â–¼                  â–¼
                        Triton path         Fallback path
                      (kernels/conv2d.py)   (F.conv2d)
                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼                  â–¼
               Stage-1 Prescan    Stage-2 Sparse Conv
               (find nz blocks)   (compute only nz)
```

## Operator Support

| Priority | Operator | Status |
|----------|----------|--------|
| P0 | Conv2d 3Ã—3 (stride=1, groups=1) | âœ… Implemented |
| P0 | Conv2d 1Ã—1 (stride=1, groups=1) | âœ… Implemented |
| P1 | Linear | ğŸ”œ Planned |
| P1 | BatchNorm2d | ğŸ”œ Planned |
| P2 | Conv2d depthwise | ğŸ”œ Planned |
| P2 | MultiheadAttention | ğŸ”œ Planned |
| P2 | ConvTranspose2d | ğŸ”œ Planned |

## How It Works

### Block Size Selection

SparseFlow automatically selects the prescan block size based on feature map spatial dimensions:

| Feature Map Size | Block Size | Typical Layer |
|-----------------|-----------|---------------|
| H â‰¥ 56 | 16 | layer1 (56Ã—56), layer2 (28Ã—28) |
| 14 â‰¤ H < 56 | 8 | layer3 (14Ã—14) |
| H â‰¤ 7 | Skip | layer4 (7Ã—7), too small to benefit |

### Two-Stage Kernel Design

**Stage-1 (Prescan):** A lightweight kernel scans every (N, C, block_h, block_w) tile. If all values in the tile are below a threshold (default 1e-6), the tile is marked as zero. Output: a compact list of non-zero block indices.

**Stage-2 (Sparse Compute):** Only non-zero blocks are dispatched to the convolution kernel. Each block loads the relevant input region, multiplies by the convolution weights, and accumulates to the output via atomic adds. Zero blocks are never touched.

### SparseConv2d Module

`SparseConv2d` is a drop-in replacement for `torch.nn.Conv2d`:

```python
from sparseflow.ops import SparseConv2d

# Create from existing Conv2d (copies weights)
sparse_conv = SparseConv2d.from_dense(original_conv, block_size=16)

# Or use directly
sparse_conv = SparseConv2d(64, 128, kernel_size=3, padding=1, block_size=16)
```

Features:
- Handles both 4D `(N,C,H,W)` and 5D `(T,N,C,H,W)` inputs (spikingjelly multi-step format)
- Automatic fallback to `F.conv2d` when Triton/CUDA is unavailable
- Records per-forward timing for profiling via `module._last_sparse_ms`

## Requirements

- Python 3.8+
- PyTorch 2.0+
- Triton 2.0+ (for GPU acceleration)
- NVIDIA GPU (Triton-supported architecture)
- [spikingjelly](https://github.com/fangwei123456/spikingjelly) (for SNN model support)

## Project Status

- [x] 3Ã—3 Conv2d sparse kernel with real weights
- [x] 1Ã—1 Conv2d sparse kernel with real weights
- [x] `SparseConv2d` nn.Module wrapper
- [x] Core framework (registry, analyzer, replacer)
- [x] `sparseflow.optimize()` top-level API
- [x] Sparsity analysis on ResNet18/34/50/101
- [ ] Numerical correctness validation on GPU
- [ ] Linear kernel
- [ ] Performance benchmarking vs cuDNN
- [ ] pip-installable package